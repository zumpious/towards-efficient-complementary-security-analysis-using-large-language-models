{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPOTBUGS_DATASET_PATH = '../../owasp_benchmark/spotbugs_dataset.pkl'\n",
    "spotbugs_dataset = pd.read_pickle(SPOTBUGS_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prefix \"cot\" marks the Chain-of-Thought prompting results, while \"default\" marks the default prompting results\n",
    "\n",
    "filenames = [\n",
    "  \"experiment_2_config2__0_shot_CoT__gpt-35-turbo_2024.06.14_21-39-08_1562.json\",\n",
    "  \"experiment_2_config2__3_shot_CoT__gpt-35-turbo_2024.06.16_17-22-11_1557.json\",\n",
    "  \"experiment_2_config2__5_shot_CoT__gpt-35-turbo_2024.06.16_19-18-11_1557.json\",\n",
    "  \"experiment_2_config2__0_shot_Default__gpt-35-turbo_2024.06.14_20-24-10_1562.json\",\n",
    "  \"experiment_2_config2__3_shot_Default__gpt-35-turbo_2024.06.16_14-22-55_1557.json\",\n",
    "  \"experiment_2_config2__5_shot_Default__gpt-35-turbo_2024.06.16_15-35-09_1557.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cot_0_shot', 'cot_3_shot', 'cot_5_shot', 'default_0_shot', 'default_3_shot', 'default_5_shot'])\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    vulnerabilities_list = [\n",
    "        {**value, \"name\": key} for key, value in json_data[\"vulnerabilities\"].items()\n",
    "    ]\n",
    "    llm_df = pd.json_normalize(vulnerabilities_list)\n",
    "    llm_df[\"Date\"] = json_data[\"Date\"]\n",
    "\n",
    "    # Extract shot count and prompt type\n",
    "    parts = filename.split('__')\n",
    "    shot_part = parts[1].lower() if len(parts) > 1 else \"\"\n",
    "\n",
    "    # Determine shot count\n",
    "    if \"0_shot\" in shot_part:\n",
    "        number = \"0\"\n",
    "    elif \"3_shot\" in shot_part:\n",
    "        number = \"3\"\n",
    "    elif \"5_shot\" in shot_part:\n",
    "        number = \"5\"\n",
    "    else:\n",
    "        number = \"unknown\"\n",
    "\n",
    "    # Determine prompt type\n",
    "    if \"cot\" in shot_part:\n",
    "        prompt = \"cot\"\n",
    "    elif \"default\" in shot_part:\n",
    "        prompt = \"default\"\n",
    "    else:\n",
    "        prompt = \"unknown\"\n",
    "\n",
    "    key = f\"{prompt}_{number}_shot\"\n",
    "    dataframes[key] = llm_df\n",
    "\n",
    "print(dataframes.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from the spotbugs dataset\n",
    "real_vulnerability_map = dict(zip(spotbugs_dataset[\"name\"], spotbugs_dataset[\"real vulnerability\"]))\n",
    "\n",
    "# Add the \"real_vulnerability\" column to each dataframe\n",
    "for _, df in dataframes.items():\n",
    "    df[\"real_vulnerability\"] = df[\"name\"].map(real_vulnerability_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRESHOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, df in dataframes.items():\n",
    "  df['llm_vul_decision'] = df['threshold_value'] >= TRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, df in dataframes.items():\n",
    "  conditions = [\n",
    "    (df['real_vulnerability'] == True) & (df['llm_vul_decision'] == True),\n",
    "    (df['real_vulnerability'] == False) & (df['llm_vul_decision'] == True),\n",
    "    (df['real_vulnerability'] == False) & (df['llm_vul_decision'] == False),\n",
    "    (df['real_vulnerability'] == True) & (df['llm_vul_decision'] == False)\n",
    "  ]\n",
    "  choices = ['TP', 'FP', 'TN', 'FN']\n",
    "  df['llm_classification'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for cot_0_shot:\n",
      "TP    1068\n",
      "FP     493\n",
      "TN       0\n",
      "FN       1\n",
      "Name: llm_classification, dtype: int64\n",
      "\n",
      "Confusion Matrix for cot_3_shot:\n",
      "TP    1049\n",
      "FP     465\n",
      "TN      28\n",
      "FN      15\n",
      "Name: llm_classification, dtype: int64\n",
      "\n",
      "Confusion Matrix for cot_5_shot:\n",
      "TP    1032\n",
      "FP     437\n",
      "TN      56\n",
      "FN      32\n",
      "Name: llm_classification, dtype: int64\n",
      "\n",
      "Confusion Matrix for default_0_shot:\n",
      "TP    1060\n",
      "FP     482\n",
      "TN      11\n",
      "FN       9\n",
      "Name: llm_classification, dtype: int64\n",
      "\n",
      "Confusion Matrix for default_3_shot:\n",
      "TP    1001\n",
      "FP     445\n",
      "TN      48\n",
      "FN      63\n",
      "Name: llm_classification, dtype: int64\n",
      "\n",
      "Confusion Matrix for default_5_shot:\n",
      "TP    924\n",
      "FP    391\n",
      "TN    102\n",
      "FN    140\n",
      "Name: llm_classification, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompting_technique, df in dataframes.items():\n",
    "  print(f\"Confusion Matrix for {prompting_technique}:\")\n",
    "  print(df[\"llm_classification\"].value_counts().reindex(choices, fill_value=0))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the results of 3-shot and 5-shot CoT presented the best ratio of TN to FN classifications. While the ratio between the two is comparable, 5-shot CoT prompting results in more than double the number of FN classifications compared to 3-shot CoT. Furthermore, 5-shot CoT prompting is computationally more expensive, requiring approximately 54% more input tokens (+4,320,159) and therefore being nearly 46% more expensive in terms of OpenAIâ€™s API costs.\n",
    "\n",
    "For this reason, we decided to continue our experiments using the 3-shot CoT prompting technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
