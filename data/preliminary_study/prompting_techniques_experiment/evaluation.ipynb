{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPOTBUGS_DATASET_PATH = '../../owasp_benchmark/spotbugs_dataset.pkl'\n",
    "spotbugs_dataset = pd.read_pickle(SPOTBUGS_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "  \"experiment_2_config2__0_shot_CoT__gpt-35-turbo_2024.06.14_21-39-08_1562.json\",\n",
    "  \"experiment_2_config2__3_shot_CoT__gpt-35-turbo_2024.06.16_17-22-11_1557.json\",\n",
    "  \"experiment_2_config2__5_shot_CoT__gpt-35-turbo_2024.06.16_19-18-11_1557.json\",\n",
    "  \"experiment_2_config2__0_shot_Default__gpt-35-turbo_2024.06.14_20-24-10_1562.json\",\n",
    "  \"experiment_2_config2__3_shot_Default__gpt-35-turbo_2024.06.16_14-22-55_1557.json\",\n",
    "  \"experiment_2_config2__5_shot_Default__gpt-35-turbo_2024.06.16_15-35-09_1557.json\" # 5 shot default prompting results contain one invalid response (trehsold_value = -1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cot_0_shot', 'cot_3_shot', 'cot_5_shot', 'default_0_shot', 'default_3_shot', 'default_5_shot'])\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "results_directory = \"results/\"\n",
    "\n",
    "filenames = [os.path.join(results_directory, filename) for filename in filenames]\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    vulnerabilities_list = [\n",
    "        {**value, \"name\": key} for key, value in json_data[\"vulnerabilities\"].items()\n",
    "    ]\n",
    "    llm_df = pd.json_normalize(vulnerabilities_list)\n",
    "    llm_df[\"Date\"] = json_data[\"Date\"]\n",
    "\n",
    "    parts = filename.split('__')\n",
    "    shot_part = parts[1].lower() if len(parts) > 1 else \"\"\n",
    "\n",
    "    if \"0_shot\" in shot_part:\n",
    "        number = \"0\"\n",
    "    elif \"3_shot\" in shot_part:\n",
    "        number = \"3\"\n",
    "    elif \"5_shot\" in shot_part:\n",
    "        number = \"5\"\n",
    "    else:\n",
    "        number = \"unknown\"\n",
    "\n",
    "    if \"cot\" in shot_part:\n",
    "        prompt = \"cot\"\n",
    "    elif \"default\" in shot_part:\n",
    "        prompt = \"default\"\n",
    "    else:\n",
    "        prompt = \"unknown\"\n",
    "\n",
    "    key = f\"{prompt}_{number}_shot\"\n",
    "    dataframes[key] = llm_df\n",
    "\n",
    "print(dataframes.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0  \\nAnalyze the following potential vulnerabilit...   \n",
      "1  \\nAnalyze the following potential vulnerabilit...   \n",
      "2  \\nAnalyze the following potential vulnerabilit...   \n",
      "3  \\nAnalyze the following potential vulnerabilit...   \n",
      "4  \\nAnalyze the following potential vulnerabilit...   \n",
      "\n",
      "                                            response confidence_of_llm  \\\n",
      "0  Let's think step by step...\\n\\nThe identified ...     Decision: 7.0   \n",
      "1  Let's think step by step...\\n\\nThe identified ...     Decision: 9.0   \n",
      "2  Let's think step by step...\\n\\nThe identified ...     Decision: 8.0   \n",
      "3  Let's think step by step...\\n\\nThe identified ...     Decision: 7.5   \n",
      "4  Let's think step by step...\\n\\nThe vulnerabili...     Decision: 9.0   \n",
      "\n",
      "   computation_time  threshold_value  prompt_tokens  response_tokens  \\\n",
      "0          1.275691              7.0           1069              178   \n",
      "1          1.422901              9.0           1969              166   \n",
      "2          1.876267              8.0           1679              234   \n",
      "3          1.283335              7.5            742              187   \n",
      "4          1.967058              9.0           1084              224   \n",
      "\n",
      "                 name        Date  real_vulnerability  llm_vul_decision  \\\n",
      "0  BenchmarkTest00346  2024-06-14                True              True   \n",
      "1  BenchmarkTest01017  2024-06-14                True              True   \n",
      "2  BenchmarkTest01895  2024-06-14                True              True   \n",
      "3  BenchmarkTest00324  2024-06-14                True              True   \n",
      "4  BenchmarkTest02245  2024-06-14                True              True   \n",
      "\n",
      "  llm_classification  \n",
      "0                 TP  \n",
      "1                 TP  \n",
      "2                 TP  \n",
      "3                 TP  \n",
      "4                 TP  \n"
     ]
    }
   ],
   "source": [
    "print(dataframes['cot_0_shot'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_vulnerability_map = dict(zip(spotbugs_dataset[\"name\"], spotbugs_dataset[\"real vulnerability\"]))\n",
    "\n",
    "for _, df in dataframes.items():\n",
    "    df[\"real_vulnerability\"] = df[\"name\"].map(real_vulnerability_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRESHOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, df in dataframes.items():\n",
    "  df['llm_vul_decision'] = np.where(df['threshold_value'] < 0.0, True, df['threshold_value'] >= TRESHOLD) # due to handling invalid LLM responses we label threshold_value < 0 as True, as those weaknesses must be reviewed by humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, df in dataframes.items():\n",
    "  conditions = [\n",
    "    (df['real_vulnerability'] == True) & (df['llm_vul_decision'] == True),\n",
    "    (df['real_vulnerability'] == False) & (df['llm_vul_decision'] == True),\n",
    "    (df['real_vulnerability'] == False) & (df['llm_vul_decision'] == False),\n",
    "    (df['real_vulnerability'] == True) & (df['llm_vul_decision'] == False)\n",
    "  ]\n",
    "  choices = ['TP', 'FP', 'TN', 'FN']\n",
    "  df['llm_classification'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for cot_0_shot:\n",
      "TP    1063\n",
      "FP     493\n",
      "TN       0\n",
      "FN       1\n",
      "Name: llm_classification, dtype: int64\n",
      "Total samples: 1557\n",
      "Total prompt tokens: 1,566,463\n",
      "Total response tokens: 273,211\n",
      "Total tokens: 1,839,674\n",
      "----------------------------------------\n",
      "Confusion Matrix for cot_3_shot:\n",
      "TP    1049\n",
      "FP     465\n",
      "TN      28\n",
      "FN      15\n",
      "Name: llm_classification, dtype: int64\n",
      "Total samples: 1557\n",
      "Total prompt tokens: 7,969,063\n",
      "Total response tokens: 628,048\n",
      "Total tokens: 8,597,111\n",
      "----------------------------------------\n",
      "Confusion Matrix for cot_5_shot:\n",
      "TP    1032\n",
      "FP     437\n",
      "TN      56\n",
      "FN      32\n",
      "Name: llm_classification, dtype: int64\n",
      "Total samples: 1557\n",
      "Total prompt tokens: 12,289,222\n",
      "Total response tokens: 615,808\n",
      "Total tokens: 12,905,030\n",
      "----------------------------------------\n",
      "Confusion Matrix for default_0_shot:\n",
      "TP    1055\n",
      "FP     482\n",
      "TN      11\n",
      "FN       9\n",
      "Name: llm_classification, dtype: int64\n",
      "Total samples: 1557\n",
      "Total prompt tokens: 1,535,323\n",
      "Total response tokens: 93,499\n",
      "Total tokens: 1,628,822\n",
      "----------------------------------------\n",
      "Confusion Matrix for default_3_shot:\n",
      "TP    1001\n",
      "FP     445\n",
      "TN      48\n",
      "FN      63\n",
      "Name: llm_classification, dtype: int64\n",
      "Total samples: 1557\n",
      "Total prompt tokens: 5,948,995\n",
      "Total response tokens: 12,795\n",
      "Total tokens: 5,961,790\n",
      "----------------------------------------\n",
      "Confusion Matrix for default_5_shot:\n",
      "TP    925\n",
      "FP    391\n",
      "TN    102\n",
      "FN    139\n",
      "Name: llm_classification, dtype: int64\n",
      "Total samples: 1557\n",
      "Total prompt tokens: 8,905,641\n",
      "Total response tokens: 15,123\n",
      "Total tokens: 8,920,764\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompting_technique, df in dataframes.items():\n",
    "  print(f\"Confusion Matrix for {prompting_technique}:\")\n",
    "  print(df[\"llm_classification\"].value_counts().reindex(choices, fill_value=0))\n",
    "  print(f\"Total samples: {len(df)}\")\n",
    "  print(f\"Total prompt tokens: {df['prompt_tokens'].sum():,}\")\n",
    "  print(f\"Total response tokens: {df['response_tokens'].sum():,}\")\n",
    "  print(f\"Total tokens: {df['prompt_tokens'].sum() + df['response_tokens'].sum():,}\")\n",
    "  print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the results of 3-shot and 5-shot CoT presented the best ratio of TN to FN classifications. While the ratio between the two is comparable, 5-shot CoT prompting results in more than double the number of FN classifications compared to 3-shot CoT. Furthermore, 5-shot CoT prompting is computationally more expensive, requiring approximately 54% more input tokens (+4,320,159) and therefore being nearly 46% more expensive in terms of OpenAIâ€™s API costs.\n",
    "\n",
    "For this reason, we decided to continue our experiments using the 3-shot CoT prompting technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [f for f in os.listdir('self_consistency_results/') if f.endswith('.json')]\n",
    "\n",
    "dataframes_sc = {}\n",
    "def get_key(filename):\n",
    "  return os.path.splitext(filename)[0]\n",
    "\n",
    "for filename in json_files:\n",
    "    with open('self_consistency_results/' + filename, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    vulnerabilities_list = [\n",
    "        {**value, \"name\": key} for key, value in json_data[\"vulnerabilities\"].items()\n",
    "    ]\n",
    "    \n",
    "    llm_df = pd.json_normalize(vulnerabilities_list)\n",
    "    llm_df[\"Date\"] = json_data[\"Date\"]\n",
    "    dataframes_sc[get_key(filename)] = llm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['experiment_2_sc_config2__few_shot_CoT__gpt-35-turbo_2024.06.19_17-40-34_1557', 'experiment_2_sc_config2__few_shot_CoT__gpt-35-turbo_2024.06.19_21-07-00_1557', 'experiment_2_sc_config2__few_shot_CoT__gpt-35-turbo_2024.06.20_00-11-47_1557', 'experiment_2_sc_config2__few_shot_CoT__gpt-35-turbo_2024.06.19_19-26-51_1557', 'experiment_2_sc_config2__few_shot_CoT__gpt-35-turbo_2024.06.19_22-41-24_1557'])\n"
     ]
    }
   ],
   "source": [
    "print(dataframes_sc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, df in dataframes_sc.items():\n",
    "    df[\"real_vulnerability\"] = df[\"name\"].map(real_vulnerability_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRESHOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1556    True\n",
      "Name: llm_vul_decision, dtype: bool\n",
      "1556    True\n",
      "Name: llm_vul_decision, dtype: bool\n",
      "1556    False\n",
      "Name: llm_vul_decision, dtype: bool\n",
      "1556    True\n",
      "Name: llm_vul_decision, dtype: bool\n",
      "1556    True\n",
      "Name: llm_vul_decision, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "for _, df in dataframes_sc.items():\n",
    "  df['llm_vul_decision'] = np.where(df['threshold_value'] < 0.0, True, df['threshold_value'] >= TRESHOLD) # due to handling invalid LLM responses we label threshold_value < 0 as True, as those weaknesses must be reviewed by humans\n",
    "  print(df.tail(1)['llm_vul_decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc_consistent = list(dataframes_sc.values())[0].copy()\n",
    "\n",
    "dfs = list(dataframes_sc.values())\n",
    "dfs_other = [df for df in dfs if df is not list(dataframes_sc.values())[0]]\n",
    "\n",
    "for idx in df_sc_consistent.index:\n",
    "  decisions = [df.loc[idx, 'llm_vul_decision'] for df in dfs]\n",
    "  count_true = decisions.count(True)\n",
    "  count_false = decisions.count(False)\n",
    "  consensus = df_sc_consistent.loc[idx, 'llm_vul_decision'] if count_true == count_false else (True if count_true > count_false else False)\n",
    "  df_sc_consistent.at[idx, 'llm_vul_decision'] = consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, _ in dataframes_sc.items():\n",
    "  conditions = [\n",
    "    (df_sc_consistent['real_vulnerability'] == True) & (df_sc_consistent['llm_vul_decision'] == True),\n",
    "    (df_sc_consistent['real_vulnerability'] == False) & (df_sc_consistent['llm_vul_decision'] == True),\n",
    "    (df_sc_consistent['real_vulnerability'] == False) & (df_sc_consistent['llm_vul_decision'] == False),\n",
    "    (df_sc_consistent['real_vulnerability'] == True) & (df_sc_consistent['llm_vul_decision'] == False)\n",
    "  ]\n",
    "  choices = ['TP', 'FP', 'TN', 'FN']\n",
    "  df_sc_consistent['llm_classification'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for SC dataframe\n",
      "TP    1055\n",
      "FP     467\n",
      "TN      26\n",
      "FN       9\n",
      "Name: llm_classification, dtype: int64\n",
      "1557\n"
     ]
    }
   ],
   "source": [
    "print(f\"Confusion Matrix for SC dataframe\")\n",
    "print(df_sc_consistent[\"llm_classification\"].value_counts().reindex(choices, fill_value=0))\n",
    "print(len(df_sc_consistent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
